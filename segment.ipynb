{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.crop import postprocess_mask\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "# from torchmetrics import Dice\n",
    "\n",
    "# !pip install git+https://github.com/qubvel/segmentation_models.pytorch\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths\n",
    "ROOT_DIR = Path(r'E:\\Prut\\cxr\\data\\segmentation\\data\\Lung Segmentation')\n",
    "TRAIN_IMG_DIR = ROOT_DIR / 'CXR_png'\n",
    "TRAIN_MASKS_DIR = ROOT_DIR / 'masks'\n",
    "\n",
    "# Hyperparameters\n",
    "SEED = 1234\n",
    "IMG_SIZE = 224\n",
    "TEST_SIZE = 0.2\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "CONTRAST_FACTOR = 1.8\n",
    "\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 1e-2\n",
    "PATIENCE = 2\n",
    "MIN_DELTA = 1e-3\n",
    "GAMMA = 0.5\n",
    "\n",
    "\n",
    "\n",
    "for dir in [TRAIN_IMG_DIR, TRAIN_MASKS_DIR]:\n",
    "    print('Length of', dir.stem, len(os.listdir(dir)))\n",
    "\n",
    "TRAIN_FILE_NAMES = sorted(set(os.listdir(TRAIN_IMG_DIR)) & set(os.listdir(TRAIN_MASKS_DIR)))\n",
    "print('Files to be used:', len(TRAIN_FILE_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = TRAIN_FILE_NAMES[6]\n",
    "img = cv2.resize(cv2.imread(str(TRAIN_IMG_DIR / path)), (IMG_SIZE, IMG_SIZE)) # [:,:,0]\n",
    "mask = cv2.resize(cv2.imread(str(TRAIN_MASKS_DIR / path)), (IMG_SIZE, IMG_SIZE)) # [:,:,0]\n",
    "added = cv2.addWeighted(img, 0.7, mask, 0.3, 0)\n",
    "stacked = np.hstack((img, mask, added))\n",
    "Image.fromarray(stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "test = random.sample(TRAIN_FILE_NAMES, round(len(TRAIN_FILE_NAMES) * TEST_SIZE))\n",
    "train = sorted(set(TRAIN_FILE_NAMES) - set(test))\n",
    "assert set(train) & set(test) == set()\n",
    "print('Length of whole dataset: {}'.format(len(TRAIN_FILE_NAMES)))\n",
    "print('Length of train dataset: {}'.format(len(train)))\n",
    "print('Length of test dataset: {}'.format(len(test)))\n",
    "\n",
    "# X_train = [TRAIN_IMG_DIR / stem for stem in train]\n",
    "# y_train = [TRAIN_MASKS_DIR / stem for stem in train]\n",
    "# X_test = [TRAIN_IMG_DIR / stem for stem in test]\n",
    "# y_test = [TRAIN_MASKS_DIR / stem for stem in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastTransform:\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return TF.adjust_contrast(x, self.factor)\n",
    "\n",
    "class SegDataset(Dataset):\n",
    "    def __init__(self, paths, visualise=False):\n",
    "        self.paths = paths\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.ColorJitter(0.1, 0, 0.1, 0.1), # Added augmentation 18-08-23\n",
    "            ContrastTransform(CONTRAST_FACTOR),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            # # No idea why I needed the next 2 lines, but otherwise the range wouldn't be [0,1] but something like [-2.5,1.5]\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(), # transforms.Lambda(lambda x: x/255.)\n",
    "            \n",
    "        ])\n",
    "        self.mask_transform = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Grayscale(num_output_channels=1)\n",
    "        ])\n",
    "        self.visualise = visualise\n",
    "        self.pil = transforms.ToPILImage()\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(str(TRAIN_IMG_DIR / self.paths[idx]))\n",
    "        mask = Image.open(str(TRAIN_MASKS_DIR / self.paths[idx]))\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "            mask = self.mask_transform(mask)\n",
    "            mask = torch.where(mask != 0, torch.tensor(1), mask)\n",
    "            mask = mask.to(torch.float32)\n",
    "\n",
    "        if self.visualise:\n",
    "            img = self.pil(img)\n",
    "            mask = self.pil(mask)\n",
    "        \n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_dataset = SegDataset(train, visualise=True)\n",
    "visualise_dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train\n",
    "train_dataset = SegDataset(train)\n",
    "test_dataset = SegDataset(test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_dataset[0][0].min().item() >= 0.\n",
    "assert train_dataset[0][0].max().item() <= 1.\n",
    "assert train_dataset[0][0].size(0) == 1\n",
    "assert train_dataset[0][0].shape == train_dataset[0][1].shape == torch.Size([1, 224, 224])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try if the shape works on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/qubvel/segmentation_models.pytorch\n",
    "# import segmentation_models_pytorch as smp\n",
    "model = smp.Unet(\"resnet34\", encoder_weights=\"imagenet\", in_channels=1, classes=2) # encoder_depth, aux_params\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Shape checking\n",
    "input = torch.randn((32, 1, 224, 224)).to(device)\n",
    "print('Model accepts input of shape:', input.shape)\n",
    "output = model(input)\n",
    "print('Model generates output of shape:', output.shape)\n",
    "\n",
    "# Try on our dataloaders\n",
    "X, y = next(iter(train_dataloader))\n",
    "X, y = X.to(device), y.to(device)\n",
    "print('Size of real input:', X.shape)\n",
    "\n",
    "pred = model(X)\n",
    "print('Size of prediction:', pred.shape)\n",
    "\n",
    "# Show result\n",
    "pil = transforms.ToPILImage()\n",
    "im1 = pil(X[0])\n",
    "im2 = pil(y[0])\n",
    "im3 = pil(pred[0][0])\n",
    "im4 = pil(pred[0][1])\n",
    "im = Image.new('RGB', (im1.width + im2.width + im3.width + im4.width, im1.height))\n",
    "im.paste(im1, (0,0))\n",
    "im.paste(im2, (im1.width, 0))\n",
    "im.paste(im3, (im1.width * 2, 0))\n",
    "im.paste(im4, (im1.width * 3, 0))\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I searched 'unet resnet outputs 2 pictures which one do i use' on chatgpt:\n",
    "```\n",
    "Segmentation Map: The first output picture represents the segmentation map, which is a pixel-wise prediction map that assigns a class label to each pixel in the input image. This map indicates which class or category each pixel belongs to. The pixel values in this map are usually integers representing class labels, or in some cases, the probabilities of each class.\n",
    "\n",
    "Auxiliary Output: The second output picture is often an auxiliary output that serves as an intermediate representation. This output is not always present in all U-Net with ResNet implementations, and its purpose can vary based on the specific architecture or task. Sometimes, this auxiliary output is used for regularization or for providing additional supervision during training.\n",
    "```\n",
    "It says use segmentation map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred.shape)\n",
    "print(pred[:,0,:,:].unsqueeze(1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Accuracy:', accuracy_fn(torch.tensor([1,0,0,0,1]), torch.tensor([0,0,1,1,1]))) # 0.4\n",
    "# print('Loss:', loss_fn(torch.tensor([1,0,0,0,1]), torch.tensor([0,0,1,1,1]))) # 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(preds, target, smooth=1e-6, round=True):\n",
    "    \n",
    "    preds = torch.flatten(preds)\n",
    "    if round:\n",
    "        preds = torch.round(preds)\n",
    "    target = torch.flatten(target)\n",
    "    intersection = (preds * target).sum()\n",
    "    dice = (2. * intersection + smooth) / (preds.sum() + target.sum() + smooth)\n",
    "    return dice\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, preds, target, smooth=1e-6):\n",
    "\n",
    "        preds = torch.flatten(preds)\n",
    "        target = torch.flatten(target)\n",
    "        \n",
    "        intersection = (preds * target).sum()\n",
    "        dice = (2. * intersection + smooth) / (preds.sum() + target.sum() + smooth) \n",
    "\n",
    "        loss = 1 - dice\n",
    "\n",
    "        # assert loss.grad_fn is not None # if train\n",
    "        return loss\n",
    "    \n",
    "class EarlyStopper:\n",
    "    '''https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch'''\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def train_step(dataloader, model, loss_fn, optimizer, accuracy_fn):\n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        preds = model(X)\n",
    "        preds = preds[:,0,:,:].unsqueeze(1)\n",
    "\n",
    "        loss = loss_fn(preds, y)\n",
    "\n",
    "        acc = accuracy_fn(preds, y)\n",
    "\n",
    "        if batch % 4 == 1:\n",
    "            print(f'Batch {batch}: Loss {loss:.6f} | Acc {acc * 100:.1f}%')\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss * X.size(0)\n",
    "        train_accuracy += acc * X.size(0)\n",
    "        total_samples += X.size(0)\n",
    "\n",
    "    train_loss /= total_samples\n",
    "    train_accuracy /= total_samples\n",
    "\n",
    "    return model, train_loss, train_accuracy\n",
    "\n",
    "def test_step(dataloader, model, loss_fn, accuracy_fn):\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            preds = model(X)\n",
    "            preds = preds[:,0,:,:].unsqueeze(1)\n",
    "\n",
    "            loss = loss_fn(preds, y)\n",
    "            acc = accuracy_fn(preds, y)\n",
    "\n",
    "            test_loss += loss * X.size(0)\n",
    "            test_accuracy += acc * X.size(0)\n",
    "            total_samples += X.size(0)\n",
    "\n",
    "        test_loss /= total_samples\n",
    "        test_accuracy /= total_samples\n",
    "    \n",
    "    print(f'Loss {loss:.6f} | Dice Accuracy {acc * 100:.1f}%')\n",
    "\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "\n",
    "# ---------- START HERE ----------\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies =[]\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "model_l = []\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Model\n",
    "model = smp.Unet(\"resnet34\", encoder_weights=\"imagenet\", in_channels=1, classes=2, activation='sigmoid').to(device)\n",
    "# Unfreeze decoder and segmentation head\n",
    "for name, param in model.named_parameters():\n",
    "    if ('segmentation' not in name) & ('decoder' not in name): \n",
    "        param.requires_grad = False\n",
    "\n",
    "# Loss, Optimizer, Accuracy functions\n",
    "loss_fn = DiceLoss().to(device)\n",
    "accuracy_fn = dice\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=GAMMA, last_epoch=-1, verbose=True) # LinearLR(optimizer, start_factor=1.0, end_factor=0.3, total_iters=5)\n",
    "early_stopper = EarlyStopper(patience=PATIENCE, min_delta=MIN_DELTA)\n",
    "\n",
    "# Training and testing loops\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "\n",
    "    # Train and test loops\n",
    "    model, train_loss, train_accuracy = train_step(dataloader=train_dataloader, model=model, loss_fn=loss_fn, optimizer=optimizer, accuracy_fn=accuracy_fn)\n",
    "    test_loss, test_accuracy = test_step(dataloader=test_dataloader, model=model, loss_fn=loss_fn, accuracy_fn=accuracy_fn)\n",
    "\n",
    "    # For overfitting visualisation\n",
    "    model_l.append(model)\n",
    "    train_losses.append(train_loss.item())\n",
    "    train_accuracies.append(train_accuracy.item())\n",
    "    test_losses.append(test_loss.item())\n",
    "    test_accuracies.append(test_accuracy.item())\n",
    "\n",
    "    # Early stopping\n",
    "    if early_stopper.early_stop(test_loss.item()):\n",
    "        # Model checkpoint (p tong said use first model before plateau)\n",
    "        model = model_l[-PATIENCE]\n",
    "        break\n",
    "\n",
    "    # Learning rate scheduler (stored in optimizer.param_groups[0][\"lr\"])\n",
    "    scheduler.step()\n",
    "\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "ax1.plot(train_losses, label='Train Loss', color='blue', linestyle='-', marker='o')\n",
    "ax1.plot(test_losses, label='Test Loss', color='red', linestyle='-', marker='x')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Train and Test Loss Over Epochs')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(train_accuracies, label='Train Accuracy', color='green', linestyle='-', marker='o')\n",
    "ax2.plot(test_accuracies, label='Test Accuracy', color='orange', linestyle='-', marker='x')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Train and Test Accuracy Over Epochs')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise after model training\n",
    "X, y = next(iter(test_dataloader))\n",
    "X, y = X.to(device), y.to(device)\n",
    "print('Size of real input:', X.shape)\n",
    "\n",
    "pred = model(X)\n",
    "print('Size of prediction:', pred.shape)\n",
    "\n",
    "# Show result\n",
    "pil = transforms.ToPILImage()\n",
    "im1 = pil(X[0])\n",
    "im2 = pil(y[0].to(torch.float32))\n",
    "im3 = pil(pred[0][0])\n",
    "im4 = pil(pred[0][1])\n",
    "im = Image.new('RGB', (im1.width + im2.width + im3.width + im4.width, im1.height))\n",
    "im.paste(im1, (0,0))\n",
    "im.paste(im2, (im1.width, 0))\n",
    "im.paste(im3, (im1.width * 2, 0))\n",
    "im.paste(im4, (im1.width * 3, 0))\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, col_names=['num_params', 'trainable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, r'E:\\Prut\\cxr\\models\\lung_segment_model_180823.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison with scores after postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_unet_list = []\n",
    "dice_postprocessed_list = []\n",
    "p = transforms.ToTensor()\n",
    "\n",
    "for stem in tqdm(test):\n",
    "\n",
    "    img = Image.open(str(TRAIN_IMG_DIR / stem))\n",
    "    mask_truth = Image.open(str(TRAIN_MASKS_DIR / stem)).resize((224,224))\n",
    "    mask_unet = postprocess_mask(img, model, return_original=True)\n",
    "    mask_postprocessed = postprocess_mask(img, model)\n",
    "    \n",
    "    dice_unet = dice(p(mask_unet), p(mask_truth))\n",
    "    dice_postprocessed = dice(p(mask_postprocessed), p(mask_truth))\n",
    "\n",
    "    dice_unet_list.append(dice_unet)\n",
    "    dice_postprocessed_list.append(dice_postprocessed)\n",
    "\n",
    "avg_dice_unet = np.mean(dice_unet_list)\n",
    "avg_dice_postprocessed = np.mean(dice_postprocessed_list)\n",
    "increase = avg_dice_postprocessed - avg_dice_unet\n",
    "\n",
    "print(f'Average Dice score after U-Net: {avg_dice_unet * 100:.3f} %')\n",
    "print(f'Average Dice score after postprocessing: {avg_dice_postprocessed * 100:.3f} %')\n",
    "print(f'Increase in Dice score: {increase * 100:.3f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(preds, target):\n",
    "    preds = torch.flatten(preds)\n",
    "    preds = torch.round(preds)\n",
    "    target = torch.flatten(target)\n",
    "    intersection = (preds * target).sum()\n",
    "    overlap = intersection / target.sum()\n",
    "    return overlap\n",
    "\n",
    "overlap_unet_list = []\n",
    "overlap_postprocessed_list = []\n",
    "p = transforms.ToTensor()\n",
    "\n",
    "for stem in tqdm(test):\n",
    "\n",
    "    img = Image.open(str(TRAIN_IMG_DIR / stem))\n",
    "    mask_truth = Image.open(str(TRAIN_MASKS_DIR / stem)).resize((224,224))\n",
    "    mask_unet = postprocess_mask(img, model, return_original=True)\n",
    "    mask_postprocessed = postprocess_mask(img, model)\n",
    "    \n",
    "    overlap_unet = overlap(p(mask_unet), p(mask_truth))\n",
    "    overlap_postprocessed = overlap(p(mask_postprocessed), p(mask_truth))\n",
    "\n",
    "    overlap_unet_list.append(overlap_unet)\n",
    "    overlap_postprocessed_list.append(overlap_postprocessed)\n",
    "\n",
    "avg_overlap_unet = np.mean(overlap_unet_list)\n",
    "avg_overlap_postprocessed = np.mean(overlap_postprocessed_list)\n",
    "increase = avg_overlap_postprocessed - avg_overlap_unet\n",
    "\n",
    "print(f'Average overlap score after U-Net: {avg_overlap_unet * 100:.3f} %')\n",
    "print(f'Average overlap score after postprocessing: {avg_overlap_postprocessed * 100:.3f} %')\n",
    "print(f'Increase in overlap score: {increase * 100:.3f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.crop import CropLungsTransform\n",
    "# CropLungsTransform(model)(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
